{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import logging\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from senpy.plugins import EmotionPlugin, SenpyPlugin\n",
    "from senpy.models import Results, EmotionSet, Entry, Emotion\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# added packages\n",
    "import codecs, csv, re, nltk\n",
    "import numpy as np\n",
    "import math, itertools\n",
    "from drevicko.twitter_regexes import cleanString, setupRegexes, tweetPreprocessor\n",
    "import preprocess_twitter\n",
    "from collections import defaultdict\n",
    "from stop_words import get_stop_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.svm import LinearSVR, SVR\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import nltk.tokenize.casual as casual\n",
    "\n",
    "import gzip\n",
    "from datetime import datetime \n",
    "\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import load_model, model_from_json\n",
    "\n",
    "class wassaRegression(EmotionPlugin):\n",
    "    \n",
    "    def __init__(self, info, *args, **kwargs):\n",
    "        super(wassaRegression, self).__init__(info, *args, **kwargs)\n",
    "        self.name = info['name']\n",
    "        self.id = info['module']\n",
    "        self._info = info\n",
    "        local_path = os.path.dirname(os.path.abspath(__file__)) \n",
    "        \n",
    "        self._maxlen = 55   \n",
    "        self.WORD_FREQUENCY_TRESHOLD = 2\n",
    "        \n",
    "        self._savedModelPath = local_path + \"/classifiers/LSTM/wassaRegression\"\n",
    "        self._path_wordembeddings = os.path.dirname(local_path) + '/glove.twitter.27B.100d.txt.gz'\n",
    "        \n",
    "        self._paths_ngramizer = local_path + '/wassa_ngramizer.dump'\n",
    "        self._paths_svr = local_path + \"/classifiers/SVR\"\n",
    "        self._paths_linearsvr = local_path + \"/classifiers/LinearSVR\"\n",
    "        self.extension_classifier = '.dump'\n",
    "        self._paths_word_freq = os.path.join(os.path.dirname(__file__), 'wordFrequencies.dump')\n",
    "        \n",
    "#         self._emoNames = ['sadness', 'disgust', 'surprise', 'anger', 'fear', 'joy'] \n",
    "        self._emoNames = ['anger','fear','joy','sadness']      \n",
    "        \n",
    "        \n",
    "\n",
    "    def activate(self, *args, **kwargs):\n",
    "        \n",
    "        np.random.seed(1337)\n",
    "        \n",
    "#         st = datetime.now()\n",
    "#         self._wordFrequencies = self._load_unique_tokens(filename = self._paths_word_freq)\n",
    "#         logger.info(\"{} {}\".format(datetime.now() - st, \"loaded _wordFrequencies\"))\n",
    "        \n",
    "        self._wassaRegressionDLModels = {emo:self._load_model_emo_and_weights(self._savedModelPath, emo) for emo in self._emoNames}  \n",
    "                                   \n",
    "        st = datetime.now()\n",
    "        self._stop_words = get_stop_words('en')\n",
    "        logger.info(\"{} {}\".format(datetime.now() - st, \"loaded _stop_words\"))\n",
    "\n",
    "        st = datetime.now()\n",
    "        self._ngramizer = joblib.load(self._paths_ngramizer)\n",
    "        logger.info(\"{} {}\".format(datetime.now() - st, \"loaded _ngramizers\"))\n",
    "\n",
    "        self._wassaRegressionSVMmodels = {\n",
    "            #'LinearSVR': self._load_classifier(PATH=self._paths_linearsvr, ESTIMATOR='LinearSVR' ),\n",
    "            'SVR': self._load_classifier(PATH=self._paths_svr, ESTIMATOR='SVR')\n",
    "            }\n",
    "        \n",
    "        st = datetime.now()\n",
    "        self._Dictionary, self._Indices = self._load_original_vectors(\n",
    "            filename = self._path_wordembeddings, \n",
    "            sep = ' ',\n",
    "            wordFrequencies = None,#self._wordFrequencies, \n",
    "            zipped = True) # leave wordFrequencies=None for loading the entire WE file\n",
    "        logger.info(\"{} {}\".format(datetime.now() - st, \"loaded _wordEmbeddings\"))\n",
    "\n",
    "        logger.info(\"wassaRegression plugin is ready to go!\")\n",
    "        \n",
    "    def deactivate(self, *args, **kwargs):\n",
    "        try:\n",
    "            logger.info(\"wassaRegression plugin is being deactivated...\")\n",
    "        except Exception:\n",
    "            print(\"Exception in logger while reporting deactivation of wassaRegression\")\n",
    "\n",
    "            \n",
    "            \n",
    "    # CUSTOM FUNCTIONS\n",
    "    \n",
    "    def _text_preprocessor(self, text):\n",
    "        \n",
    "        text = preprocess_twitter.tokenize(text)\n",
    "        \n",
    "        text = casual.reduce_lengthening(text)\n",
    "        text = cleanString(setupRegexes('twitterProAna'),text)  \n",
    "        text = ' '.join([span for notentity,span in tweetPreprocessor(text, (\"urls\", \"users\", \"lists\")) if notentity]) \n",
    "        text = text.replace('\\t','')\n",
    "        text = text.replace('< ','<').replace(' >','>')\n",
    "        text = text.replace('):', '<sadface>').replace('(:', '<smile>')\n",
    "        text = text.replace(\" 't\", \"t\").replace(\"#\", \"\")\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def tokenise_tweet(text):\n",
    "        text = preprocess_twitter.tokenize(text)\n",
    "        text = preprocess_tweet(text)     \n",
    "        return ' '.join(text.split()) \n",
    "    \n",
    "    def _load_original_vectors(self, filename = 'glove.27B.100d.txt', sep = ' ', wordFrequencies = None, zipped = False):\n",
    "       \n",
    "        def __read_file(f):\n",
    "            Dictionary, Indices  = {},{}\n",
    "            i = 1\n",
    "            for line in f:\n",
    "                line_d = line.decode('utf-8').split(sep)\n",
    "\n",
    "                token = line_d[0]\n",
    "                token_vector = np.array(line_d[1:], dtype = 'float32')   \n",
    "                if wordFrequencies != None:\n",
    "                    if token in wordFrequencies:                \n",
    "                        Dictionary[token] = token_vector\n",
    "                        Indices.update({token:i})\n",
    "                        i+=1\n",
    "                else:\n",
    "                    Dictionary[token] = token_vector\n",
    "                    Indices.update({token:i})\n",
    "                    i+=1\n",
    "            return(Dictionary, Indices)\n",
    "            \n",
    "        if zipped:\n",
    "            with gzip.open(filename, 'rb') as f:\n",
    "                return(__read_file(f))\n",
    "        else:\n",
    "            with open(filename, 'rb') as f:\n",
    "                return(__read_file(f))    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # ===== SVR\n",
    "    \n",
    "    def _tweetToNgramVector(self, text):        \n",
    "        return self._ngramizer.transform([text,text]).toarray()[0]       \n",
    "\n",
    "    def _tweetToWordVectors(self, tweet, fixedLength=False):\n",
    "        output = []    \n",
    "        if fixedLength:\n",
    "            for i in range(100):\n",
    "                output.append(blankVector)\n",
    "            for i,token in enumerate(tweet.split()):\n",
    "                if token in self._Dictionary:\n",
    "                    output[i] = self._Dictionary[token]                \n",
    "        else:\n",
    "             for i,token in enumerate(tweet.lower().split()):\n",
    "                if token in self._Dictionary:\n",
    "                    output.append(self._Dictionary[token])            \n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def _ModWordVectors(self, x, mod=True):\n",
    "        if len(x) == 0:       \n",
    "            if mod:\n",
    "                return np.zeros(self.EMBEDDINGS_DIM*3, dtype='float32')\n",
    "            else:\n",
    "                return np.zeros(self.EMBEDDINGS_DIM, dtype='float32')\n",
    "        m = np.matrix(x)\n",
    "        if mod:\n",
    "            xMean = np.array(m.mean(0))[0]\n",
    "            xMin = np.array(m.min(0))[0]\n",
    "            xMax = np.array(m.max(0))[0]\n",
    "            xX = np.concatenate((xMean,xMin,xMax))\n",
    "            return xX\n",
    "        else:\n",
    "            return np.array(m.mean(0))[0]\n",
    "        \n",
    "    def _bindTwoVectors(self, x0, x1):\n",
    "        return np.array(list(itertools.chain(x0,x1)),dtype='float32') \n",
    "    \n",
    "    def _bind_vectors(self, x):\n",
    "        return np.concatenate(x)  \n",
    "    \n",
    "    def _load_classifier(self, PATH, ESTIMATOR):\n",
    "        \n",
    "        models = []\n",
    "        st = datetime.now()\n",
    "\n",
    "        for EMOTION in self._emoNames:\n",
    "            filename = os.path.join(PATH, EMOTION + self.extension_classifier)\n",
    "            st = datetime.now()\n",
    "            m = joblib.load(filename)\n",
    "            logger.info(\"{} loaded _wassaRegression.{}.{}\".format(datetime.now() - st, ESTIMATOR, EMOTION))\n",
    "            models.append( m )\n",
    "            \n",
    "        return models\n",
    "    \n",
    "    def _load_unique_tokens(self, filename = 'wordFrequencies.dump'):    \n",
    "        return joblib.load(filename)\n",
    "        \n",
    "    def _convert_text_to_vector(self, text, text_input):\n",
    "        \n",
    "        ngramVector = self._tweetToNgramVector(text)\n",
    "        embeddingsVector = self._ModWordVectors(self._tweetToWordVectors(text))\n",
    "        \n",
    "        X = np.asarray( self._bind_vectors((ngramVector, embeddingsVector)) ).reshape(1,-1)   \n",
    "        return X\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ===== LSTM\n",
    "    \n",
    "    def _load_model_emo_and_weights(self, filename, emo):\n",
    "        st = datetime.now()\n",
    "        with open(filename+'.'+emo+'.json', 'r') as json_file:\n",
    "            loaded_model_json = json_file.read()\n",
    "            loaded_model = model_from_json(loaded_model_json)\n",
    "            \n",
    "        loaded_model.load_weights(filename+'.'+emo+'.h5')\n",
    "        logger.info(\"{} {}\".format(datetime.now() - st, \"loaded _wassaRegression.LSTM.\"+emo))\n",
    "        return loaded_model\n",
    "    \n",
    "    def _lists_to_vectors(self, text):\n",
    "        train_sequences = [self._text_to_sequence(text)]  \n",
    "        X = sequence.pad_sequences(train_sequences, maxlen=self._maxlen)\n",
    "\n",
    "        return X\n",
    "    \n",
    "    def _text_to_sequence(self,text):\n",
    "        train_sequence = []\n",
    "        for token in text.split():\n",
    "            try:\n",
    "                train_sequence.append(self._Indices[token])\n",
    "            except:\n",
    "                train_sequence.append(0)\n",
    "        train_sequence.extend([0]*( self._maxlen-len(train_sequence)) )\n",
    "        return np.array(train_sequence)                  \n",
    "\n",
    "    def _extract_features(self, X):  \n",
    "        feature_set = {}\n",
    "        for emo in self._emoNames:\n",
    "            feature_set.update({emo:self._wassaRegressionDLModels[emo].predict(X)[0][0]})\n",
    "            \n",
    "        return feature_set \n",
    "    \n",
    "    def _extract_features_svr(self, X):  \n",
    "        feature_set = {\n",
    "            emo: float(clf.predict(X)[0]) for emo,clf in zip(self._emoNames, self._wassaRegressionSVMmodels['SVR'])}\n",
    "        return feature_set \n",
    "    \n",
    "    \n",
    "    # ANALYZE    \n",
    "    \n",
    "    def analyse(self, **params):\n",
    "        logger.debug(\"wassaRegression LSTM Analysing with params {}\".format(params))          \n",
    "        \n",
    "        st = datetime.now()\n",
    "           \n",
    "        text_input = params.get(\"input\", None)        \n",
    "        text = self._text_preprocessor(text_input) \n",
    "        \n",
    "        self.ESTIMATOR = params.get(\"estimator\", 'LSTM')\n",
    "        \n",
    "        if self.ESTIMATOR == 'LSTM':        \n",
    "            X_lstm = self._lists_to_vectors(text = text)           \n",
    "            feature_text = self._extract_features(X_lstm) \n",
    "            \n",
    "        elif self.ESTIMATOR == 'averaged':\n",
    "            X_lstm = self._lists_to_vectors(text = text)\n",
    "            X_svr = self._convert_text_to_vector(text=text, text_input=text_input) \n",
    "            \n",
    "            feature_text_lstm = self._extract_features(X_lstm)\n",
    "            feature_text_svr = self._extract_features_svr(X_svr) \n",
    "            \n",
    "            feature_text = {emo:np.mean([feature_text_lstm[emo], feature_text_svr[emo]]) for emo in self._emoNames}\n",
    "            \n",
    "        else:     \n",
    "            X_svr = self._convert_text_to_vector(text=text, text_input=text_input)            \n",
    "            feature_text = self._extract_features_svr(X_svr)  \n",
    "        \n",
    "        logger.info(\"{} {}\".format(datetime.now() - st, \"string analysed\"))\n",
    "            \n",
    "        response = Results()\n",
    "       \n",
    "        entry = Entry()\n",
    "        entry.nif__isString = text_input\n",
    "        \n",
    "        emotionSet = EmotionSet()\n",
    "        emotionSet.id = \"Emotions\"\n",
    "        \n",
    "        emotionSet.onyx__maxIntensityValue = float(100.0)\n",
    "        \n",
    "        emotion1 = Emotion() \n",
    "        for dimension in ['V','A','D']:\n",
    "            weights = [feature_text[i] for i in feature_text]\n",
    "            if not all(v == 0 for v in weights):\n",
    "                value = np.average([self.centroids[i][dimension] for i in feature_text], weights=weights) \n",
    "            else:\n",
    "                value = 5.0\n",
    "            emotion1[self.centroid_mappings[dimension]] = value      \n",
    "\n",
    "        emotionSet.onyx__hasEmotion.append(emotion1)    \n",
    "        \n",
    "        for i in feature_text:\n",
    "            emotionSet.onyx__hasEmotion.append(Emotion(onyx__hasEmotionCategory=self.wnaffect_mappings[i],\n",
    "                                    onyx__hasEmotionIntensity=float(feature_text[i])*emotionSet.onyx__maxIntensityValue))\n",
    "        \n",
    "        entry.emotions = [emotionSet,]\n",
    "        \n",
    "        response.entries.append(entry)  \n",
    "            \n",
    "        return response\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
